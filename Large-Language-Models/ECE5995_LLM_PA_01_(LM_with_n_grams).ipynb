{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AG4_Tf0Tm3Ko"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNlUCMnCom"
      },
      "source": [
        "\n",
        "**Submission Deadline**: Feb 07, 2024; 11:59 PM\n",
        "\n",
        "Welcome to the programming assignments (PAs) for *ECE 5995: Large Language Models*. For the PAs, we'll be using Python. Python has fantastic community support and have posted several resources in ICON to help you better familiarize with it.\n",
        "\n",
        "**Learning Objectives**\n",
        "\n",
        "In this assignment, we will perform language modeling using the *WikiText* dataset, which contains a large collection of text from Wikipedia articles. The task involves training models to generate coherent and contextually appropriate text, making it a fundamental problem in natural language processing.\n",
        "\n",
        "**Writing Code**\n",
        "\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space. Feel free to add and delete arguments in function signatures, but be careful that you might need to change them in function calls which are already present in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xK9bITzr0tm"
      },
      "source": [
        "# 1: N-Gram Language Modeling:  \n",
        "\n",
        "In the context of n-grams, language modeling involes estimating the likelihood of a sequence of words/tokens based on the history of preceding words.\n",
        "\n",
        "\n",
        "The goal of this portion of the assignment is to implement n-gram language models for values of $n \\in [ 2, 4, 8]$, generate sample text, and calculate the perplexity of each n-gram model on the train set.\n",
        "\n",
        "*Note: The dataset for this PA is can be found in ICON. Please do not forget to upload the data folder (i.e., wiki dataset) to your notebook. *\n",
        "\n",
        "## 1.1 Data Preprocessing :\n",
        "\n",
        "###### Complete the following code block to create the tokenizer necessary for the proceeeding experiments\n",
        "\n",
        "- Create the train tokenizer with the following properties\n",
        "    - Add a special **\\<unk\\>** token to replace any out of vocabularly (OOV) tokens\n",
        "    - Replace numeric tokens with the **\\<num\\>** token\n",
        "    - Remove punctuation and symbols\n",
        "    - Ensure the tokenizer prepends a **\\<bos\\>** and appends an **\\<eos\\>** token to every sequence\n",
        "    \n",
        "**TODO**\n",
        "- Train the tokenizer on the train set of Wiki-Text\n",
        "\n",
        "- Print the vocabulary size of the tokenizer\n",
        "- **Note:** You may want to make use of the [huggingface tokenizer docs.](https://huggingface.co/docs/tokenizers/components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f5FiRxo1vyZz"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Callable\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string as string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "18UuQ9u2sBpj"
      },
      "outputs": [],
      "source": [
        "import tokenizers\n",
        "from tokenizers.pre_tokenizers import WhitespaceSplit, Sequence\n",
        "from tokenizers import Tokenizer, normalizers, Regex\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SCk914cRuP1C"
      },
      "outputs": [],
      "source": [
        "# define the tokenizer with the help of the huggingface docs:\n",
        "# https://huggingface.co/docs/tokenizers/index\n",
        "# more tokenizers\n",
        "from tokenizers.pre_tokenizers import Digits, Punctuation\n",
        "from tokenizers.normalizers import Lowercase, Replace\n",
        "\n",
        "def train_tokenizer(fname: str) -> tokenizers.Tokenizer:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        fname: the name of the wiki.txt file\n",
        "\n",
        "    Returns: Huggingface Tokenizer\n",
        "    \"\"\"\n",
        "    PAD_TOKEN = '<pad>'\n",
        "    UNK_TOKEN = '<unk>'\n",
        "    NUM_TOKEN = '<num>'\n",
        "    START_TOKEN= '<bos>'\n",
        "    END_TOKEN= '<eos>'\n",
        "    tokenizer = Tokenizer(WordLevel(unk_token=UNK_TOKEN))\n",
        "    # =============================\n",
        "    # TODO:\n",
        "\n",
        "\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGWPK8oAnnKq"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# TODO: Train the tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN3gdCoBugQe"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# TODO: Print the Vocab size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6lRNfPLuqq1"
      },
      "outputs": [],
      "source": [
        "def sanity_check(tokenizer: tokenizers.Tokenizer,\n",
        "                sample_text: str):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokens = tokenizer.encode(sample_text).tokens\n",
        "        assert tokens[0] == '<bos>'\n",
        "        assert tokens[-1] == '<eos>'\n",
        "        assert len(tokens) == len(sample_text.split(' ')) + 2\n",
        "        assert all(token.islower() for token in tokens)\n",
        "        print('Sanity Check Passed')\n",
        "        print(tokens)\n",
        "    except AssertionError as e:\n",
        "        print('Tokenizer failed sanity check')\n",
        "        print(tokens)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xykr26oHuspF"
      },
      "outputs": [],
      "source": [
        "sample_text= 'The quick brown233 133 fox jumped over the lazy dog!'\n",
        "sanity_check(tokenizer, sample_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQUI3n1IuwqU"
      },
      "source": [
        "# 1.2 Train the N-Gram Model :  \n",
        "\n",
        "If the tokenizer passed the basic sanity check then proceed.\n",
        "\n",
        "**TODO**\n",
        " - Train n-gram models for $n \\in \\{ 2, 4, 6, 8, 10\\}$\n",
        " - Complete the *get_ngrams* function below, to return a list of all the n-grams\n",
        "     - Each entry in this list represents all the n-grams for a given sentence\n",
        " - For each n-gram model, fit the model to it respective set of n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKOEAdtiu2d3"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(fname: str,\n",
        "                  tokenizer: tokenizers.Tokenizer,\n",
        "                    n: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        fname:\n",
        "\n",
        "    Returns:\n",
        "        all_ngrams: list of all n-grams\n",
        "        for all sentences\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "    # =============================\n",
        "    # TODO: implement the function\n",
        "    #       to get the ngrams for\n",
        "    #       all the sentences\n",
        "\n",
        "\n",
        "    return all_ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSTkZSgZvCtz"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# TODO: Train the n-gram models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vfg2HP-wOrG"
      },
      "source": [
        "# 1.3  Compute N-Gram Perplexity\n",
        "\n",
        "### Perplexity is a measure of how well a given distribution predicts a sample. In the context of language modeling, the perplexity is based on how well the model predicts a given corpus. For the n-gram model, nltk provides a [function which computes the perplexity](https://www.nltk.org/api/nltk.lm.api.html#nltk.lm.api.LanguageModel.perplexity).\n",
        "\n",
        "**TODO**\n",
        "- $\\text{For n} \\in \\{2, 4, 6, 8 ,1 0\\}$, compute the perplexity of each n-gram model on the train set\n",
        "- You will need the $\\texttt{get_ngrams}$ function in your $\\texttt{ngram_perplexity}$ function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEPoqZEPwdXr"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "def ngram_perplexity(fname: str,\n",
        "                     n:int,\n",
        "                    tokenizer: tokenizers.Tokenizer,\n",
        "                    ) -> float:\n",
        "    \"\"\"\n",
        "    Arg:\n",
        "\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    # =============================\n",
        "    # TODO: Implement the function\n",
        "    #       to compute perplexity for a given\n",
        "    #       n-gram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-CoII3jwgio"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# TODO: compute the perplexity for varying values of n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04hAZmZfw7lr"
      },
      "source": [
        "# 1.4 Plot N-gram Size vs Perplexity :\n",
        "\n",
        "**TODO**:\n",
        " - $\\text{For n} \\in \\{2, 4, 6, 8 ,1 0\\}$ plot the N-Gram size vs Perplexity for each of the n-gram models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knQ_iyoWw_Ow"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# TODO: create the plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpFbIsgP5jIk"
      },
      "source": [
        "**TODO**: In the Markdown cell below, explain the effect you observe, why do you think this is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4j8hrId5rQC"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
